{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import spacy\n",
    "import scipy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from stop_words import get_stop_words\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разреженные признаки\n",
    "\n",
    "**Примеры** источников разреженных признаков:\n",
    "\n",
    "* Категориальные признаки \n",
    "* Текст\n",
    "\n",
    "## Категориальные признаки\n",
    "\n",
    "Категориальные признаки, также могут упоминаться, как факторные или номинальные признаки. \n",
    "\n",
    "**Примеры** категориальных признаков: \n",
    "\n",
    "* пол\n",
    "* страна проживания\n",
    "* номер группы\n",
    "\n",
    "и т.п.\n",
    "\n",
    "Ясно, что для компьютерной обработки вместо \"понятного для человека\" значения (в случае страны — \"Russia\", \"GB\", \"France\" и т.п.) хранят числа. Далее обсудим, как получать подобное вектор-признаки и в каком формате их хранить.\n",
    "\n",
    "Рассмотрим таблицу ```winemag.csv```, которая содержит описания вин."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>This tremendous 100% varietal wine hails from ...</td>\n",
       "      <td>Martha's Vineyard</td>\n",
       "      <td>96</td>\n",
       "      <td>235.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Heitz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>Ripe aromas of fig, blackberry and cassis are ...</td>\n",
       "      <td>Carodorum Selección Especial Reserva</td>\n",
       "      <td>96</td>\n",
       "      <td>110.0</td>\n",
       "      <td>Northern Spain</td>\n",
       "      <td>Toro</td>\n",
       "      <td></td>\n",
       "      <td>Tinta de Toro</td>\n",
       "      <td>Bodega Carmen Rodríguez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>Mac Watson honors the memory of a wine once ma...</td>\n",
       "      <td>Special Selected Late Harvest</td>\n",
       "      <td>96</td>\n",
       "      <td>90.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Knights Valley</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Sauvignon Blanc</td>\n",
       "      <td>Macauley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>This spent 20 months in 30% new French oak, an...</td>\n",
       "      <td>Reserve</td>\n",
       "      <td>96</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Ponzi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>France</td>\n",
       "      <td>This is the top wine from La Bégude, named aft...</td>\n",
       "      <td>La Brûlade</td>\n",
       "      <td>95</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Provence</td>\n",
       "      <td>Bandol</td>\n",
       "      <td></td>\n",
       "      <td>Provence red blend</td>\n",
       "      <td>Domaine de la Bégude</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country                                        description  \\\n",
       "0      US  This tremendous 100% varietal wine hails from ...   \n",
       "1   Spain  Ripe aromas of fig, blackberry and cassis are ...   \n",
       "2      US  Mac Watson honors the memory of a wine once ma...   \n",
       "3      US  This spent 20 months in 30% new French oak, an...   \n",
       "4  France  This is the top wine from La Bégude, named aft...   \n",
       "\n",
       "                            designation  points  price        province  \\\n",
       "0                     Martha's Vineyard      96  235.0      California   \n",
       "1  Carodorum Selección Especial Reserva      96  110.0  Northern Spain   \n",
       "2         Special Selected Late Harvest      96   90.0      California   \n",
       "3                               Reserve      96   65.0          Oregon   \n",
       "4                            La Brûlade      95   66.0        Provence   \n",
       "\n",
       "            region_1           region_2             variety  \\\n",
       "0        Napa Valley               Napa  Cabernet Sauvignon   \n",
       "1               Toro                          Tinta de Toro   \n",
       "2     Knights Valley             Sonoma     Sauvignon Blanc   \n",
       "3  Willamette Valley  Willamette Valley          Pinot Noir   \n",
       "4             Bandol                     Provence red blend   \n",
       "\n",
       "                    winery  \n",
       "0                    Heitz  \n",
       "1  Bodega Carmen Rodríguez  \n",
       "2                 Macauley  \n",
       "3                    Ponzi  \n",
       "4     Domaine de la Bégude  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('winemag.csv', index_col=0, na_filter=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таблице 10 столбцов с признаками. Какие из них являются категориальными? \n",
    "\n",
    "Во-первых, это должны быть столбцы содержащие текстовые значения. Следовательно, в качестве кандидатов остаются: ```country```, ```description```, ```designation```, ```province```, ```region_1```, ```region_2```, ```variety``` и ```winery```.\n",
    "\n",
    "Во-вторых, столбцы с небольшим числом уникальных значений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country: 49\n",
      "description: 97821\n",
      "designation: 30622\n",
      "province: 456\n",
      "region_1: 1237\n",
      "region_2: 19\n",
      "variety: 632\n",
      "winery: 14810\n"
     ]
    }
   ],
   "source": [
    "for name in ['country', 'description', 'designation', 'province',\n",
    "             'region_1', 'region_2', 'variety', 'winery']:\n",
    "    print('%s: %d'%(name, data[name].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, страна-производитель является категориальным признаком.\n",
    "\n",
    "Самым очевидным способом кодирования является - **one-hot-кодирование**: для кодируемого категориального признака создаются $N$ новых признаков, где $N$ - число категорий. Каждый $i$-й новый признак - бинарный характеристический признак $i$-й категории.\n",
    "\n",
    "Например, страна-производитель является категориальным признаком. Воспользуемся [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) и [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) для преобразования названий стран в one-hot-вектор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries = data.country\n",
    "countries = LabelEncoder().fit_transform(countries)\n",
    "countries = OneHotEncoder().fit_transform(countries[:, np.newaxis])\n",
    "\n",
    "type(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разреженные матрицы\n",
    "\n",
    "После кодирования мы получили разреженную матрицу признаков. Существует много типов разреженных матриц, каждый из которых предоставляет разные гарантии на операции.\n",
    "\n",
    "* ```scipy.sparse.coo_matrix```\n",
    "* ```scipy.sparse.csc_matrix```\n",
    "* ```scipy.sparse.csr_matrix```\n",
    "* ```scipy.sparse.bsr_matrix```\n",
    "* ```scipy.sparse.lil_matrix```\n",
    "* ```scipy.sparse.dia_matrix```\n",
    "* ```scipy.sparse.dok_matrix```\n",
    "\n",
    "Подробнее про [устройство разреженых матрицы](http://www.netlib.org/utk/people/JackDongarra/etemplates/node372.html)\n",
    "\n",
    "### scipy.sparse.coo_matrix\n",
    "\n",
    "* Используется как хранилище данных\n",
    "* Поддерживает быструю конвертацию в любой формат\n",
    "* Не поддерживает индексацию\n",
    "* Поддерживает ограниченый набор арифметических операций\n",
    "\n",
    "### scipy.sparse.csc_matrix\n",
    "\n",
    "* Хранит данные поколоночно\n",
    "* Быстрое получение значений отдельных колонок\n",
    "\n",
    "### scipy.sparse.csr_matrix\n",
    "\n",
    "* Хранит данные построчно\n",
    "* Быстрое получение значений отдельных строк\n",
    "\n",
    "### scipy.sparse.bsr_matrix\n",
    "\n",
    "* Подходит для разреженных матриц с плотными подматрицами\n",
    "\n",
    "### scipy.sparse.lil_matrix\n",
    "\n",
    "* Подходит для создания разреженных матриц поэлементно\n",
    "* Для последующих матричных операций лучше сконвертировать в ```csr_matrix``` или ```csc_matrix```\n",
    "\n",
    "Библиотека ```scipy.sparse``` содержит методы, позволяющие работать с разреженными матрицами. Подробнее про операции с разрежеными матрицами на сайте [scipy](https://docs.scipy.org/doc/scipy/reference/sparse.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание оценки вин\n",
    "\n",
    "В качестве категориальных признаков возьмём: country, province и variety\n",
    "\n",
    "Попробуем предсказать оценку выставленную винам. Оценки в таблицы варьируются от 80 до 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [01:02, 10.62s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <td>0.144478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>province</th>\n",
       "      <td>0.155299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variety</th>\n",
       "      <td>0.146787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country; province</th>\n",
       "      <td>0.153111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country; variety</th>\n",
       "      <td>0.153392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>province; variety</th>\n",
       "      <td>0.161825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country; province; variety</th>\n",
       "      <td>0.161885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Accuracy\n",
       "country                     0.144478\n",
       "province                    0.155299\n",
       "variety                     0.146787\n",
       "country; province           0.153111\n",
       "country; variety            0.153392\n",
       "province; variety           0.161825\n",
       "country; province; variety  0.161885"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = data.points.values\n",
    "countries = OneHotEncoder().fit_transform(LabelEncoder().\\\n",
    "                                          fit_transform(data.country)[:, np.newaxis])\n",
    "provinces = OneHotEncoder().fit_transform(LabelEncoder().\\\n",
    "                                          fit_transform(data.province)[:, np.newaxis])\n",
    "varieties = OneHotEncoder().fit_transform(LabelEncoder().\\\n",
    "                                          fit_transform(data.variety)[:, np.newaxis])\n",
    "features = [('country', countries), ('province', provinces), ('variety', varieties)]\n",
    "\n",
    "names = []\n",
    "accuracy_scores = []\n",
    "for subset_features in tqdm(itertools.chain(*[list(itertools.combinations(features, n)) \n",
    "                                              for n in range(1, 4)])):\n",
    "    subset_names, subset_features = zip(*subset_features)\n",
    "    names.append('; '.join(subset_names))\n",
    "    \n",
    "    X = scipy.sparse.hstack(subset_features)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)\n",
    "    \n",
    "    lr = LogisticRegression().fit(X_train, Y_train)\n",
    "    Y_pred = lr.predict(X_test)\n",
    "\n",
    "    accuracy_scores.append(accuracy_score(Y_pred, Y_test))\n",
    "\n",
    "pd.DataFrame({'Accuracy':accuracy_scores}, index=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение признаков из текстов\n",
    "\n",
    "Перед тем как работать с текстом, его необходимо токенизировать - разбить на отдельные токены. В качестве токенов могут выступать слова, фразы, предложений и т.п. Токенизировать текст можно  помощью регулярных выражений или готовых токенизаторов. \n",
    "\n",
    "После токенизации нужно привести текст к нормальной форме. Речь идет о [стемминге и/или лемматизации](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) - это схожие процессы, используемые для обработки словоформ.\n",
    "\n",
    "Для работы лемматизации английского текста можно воспользоваться библиотекой [SpaCy]( https://spacy.io/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150930/150930 [1:41:14<00:00, 24.85it/s]  \n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "description_lemma = [' '.join([token.lemma_ for token in nlp(text)]) \n",
    "                     for text in tqdm(data.description)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this tremendous 100 % varietal wine hail from oakville and be age over three year in oak . juicy red - cherry fruit and a compelling hint of caramel greet the palate , frame by elegant , fine tannin and a subtle minty tone in the background . balanced and rewarding from start to finish , -PRON- have year ahead of -PRON- to develop further nuance . enjoy 2022–2030 .'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_lemma[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "Cоздаем вектор длиной в словарь, для каждого слова считаем количество вхождений в текст и подставляем это число на соответствующую позицию в векторе.\n",
    "\n",
    "Построим модель BOW с помощью [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 26383\n",
      "Top-10 слов: and; the; be; of; with; pron; this; wine; flavor; in\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer().fit(description_lemma)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "print('Размер словаря: %d'%len(vocabulary))\n",
    "\n",
    "description_count = vectorizer.transform(description_lemma)\n",
    "top_tokens, _ = zip(*sorted(zip(vocabulary, description_count.sum(axis=0).getA1()), \n",
    "                            key=lambda x: x[1], reverse=True)[:10])\n",
    "print('Top-10 слов: %s'%'; '.join(top_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что большая часть из топ-10 слов является не информативными - стоп-словами. Что бы они не участвовали в представление, в конструктор CountVectorizer в качестве параметра можно передать список стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 26290\n",
      "Top-10 слов: pron; wine; flavor; fruit; finish; cherry; aroma; tannin; dry; acidity\n"
     ]
    }
   ],
   "source": [
    "stop_words = get_stop_words('en')\n",
    "vectorizer = CountVectorizer(stop_words=stop_words).fit(description_lemma)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "print('Размер словаря: %d'%len(vocabulary))\n",
    "\n",
    "description_count = vectorizer.transform(description_lemma)\n",
    "top_tokens, _ = zip(*sorted(zip(vocabulary, description_count.sum(axis=0).getA1()), \n",
    "                            key=lambda x: x[1], reverse=True)[:10])\n",
    "print('Top-10 слов: %s'%'; '.join(top_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сжать векторное представление, можно \"отбросить\" редкие слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 16143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<150930x16143 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3750840 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stop_words, min_df=3).fit(description_lemma)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "print('Размер словаря: %d'%len(vocabulary))\n",
    "\n",
    "description_count = vectorizer.transform(description_lemma)\n",
    "description_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf\n",
    "\n",
    "Cлова, которые редко встречаются в корпусе (во всех рассматриваемых документах этого набора данных), но присутствуют в этом конкретном документе, могут оказаться более важными. Тогда имеет смысл повысить вес более узкотематическим словам, чтобы отделить их от общетематических. Этот подход называется [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf).\n",
    "\n",
    "Значение Tf-Idf для каждого пары документ-слово состоит из двух компонент:\n",
    "* Term frequency — логарифм встречаемости слова в документе\n",
    "\n",
    "$$tf(t, d) = \\log n_{t,d}$$\n",
    "\n",
    "* Inverse Document frequency — логарифм обратной доли документов в которых встретилось данное слово\n",
    "\n",
    "$$idf(t, D) = \\log \\frac{ \\mid D \\mid}{\\mid \\{ d_i \\in D \\mid t \\in d_i \\} \\mid}$$\n",
    "\n",
    "* Tf-Idf — кобминация tf и idf\n",
    "\n",
    "$$ TfIdf(t, d, D) = tf(t, d) * idf(t, D)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 слов: hoed; nonnenberg; consulting; coutinel; congenial; blessing; alderbrook; marriage; charlemagne; 50g\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words).fit(description_lemma)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "description_tfidf = vectorizer.transform(description_lemma)\n",
    "top_tokens, _ = zip(*sorted(zip(vocabulary, description_count.sum(axis=0).getA1()), \n",
    "                            key=lambda x: x[1], reverse=True)[:10])\n",
    "print('Top-10 слов: %s'%'; '.join(top_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание оценки вин\n",
    "\n",
    "Добавляем к категориальным признакам, признаки извлечённые из описаний сомелье."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BOW</th>\n",
       "      <td>0.385287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TfIdf</th>\n",
       "      <td>0.325215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy\n",
       "BOW    0.385287\n",
       "TfIdf  0.325215"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_scores = []\n",
    "for description in [description_count, description_tfidf]:\n",
    "\n",
    "    X = scipy.sparse.hstack([countries, provinces, varieties, description])\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)\n",
    "    \n",
    "    lr = LogisticRegression().fit(X_train, Y_train)\n",
    "    Y_pred = lr.predict(X_test)\n",
    "    \n",
    "    accuracy_scores.append(accuracy_score(Y_pred, Y_test))\n",
    "\n",
    "pd.DataFrame({'Accuracy':accuracy_scores}, index=['BOW', 'TfIdf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25\n",
    "\n",
    "Метод вычисления весов ```Okapi BM25```, который развивает идею Tf-Idf, учитывая длину документов в $tf(t, d)$.\n",
    "\n",
    "$$tf(t, d) = \\frac{(k_1 + 1) * n_{t,d}}{k_1 * (1 - b + b * \\frac{L_d}{L_{ave}}) + n_{t,d}}$$\n",
    "\n",
    "где\n",
    "* $k_1$ и $b$ - свободные коэффициенты, обычно их выбирают как $k_1=2.0$ и $b=0.75$\n",
    "* $L_d$ - длина документа $d$\n",
    "* $L_{ave}$ - средняя длина документа в коллекции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vowpal Wabbit\n",
    "\n",
    "Довольно часто бывает, что данных очень много и обучаться приходится на выборках, которые не помещаются в память. А также, довольно часто хорошее качество можно получить благодаря простым линейным моделям, при условии, что были хорошо отобраны и сгенерированы признаки. \n",
    "\n",
    "Важным достоинством линейных методов является то, что при обучении можно добиться того, что настройка параметров алгоритма (т.е. этап обновления весов) будет производится каждый раз при добавлении нового обьекта. Данные методы машинного обучения в литературе часто также называют Online Machine Learning. При этом не нужно хранить все обьекты одновременно в памяти теперь просто не нужно.\n",
    "\n",
    "На сегодняшний день одной из самых известных реализаций таких методов является пакет [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit):\n",
    "\n",
    "* Можно обучать только линейные модели. Увеличивать качество методов, можно за счет добавления новых признаков и подгонки функции потерь\n",
    "\n",
    "* Обучающая выборка обрабатывается с помощью стахостического оптимизатора, благодаря чему можно обучаться на выборках, которые не помещаются в память\n",
    "\n",
    "* Можно обрабатывать большое количество признаков за счет их хэширования (так называемый hashing trick), бладаря чему можно обучать модели даже в случаях, когда полный набор весов просто не помещается в памяти\n",
    "\n",
    "* Поддерживается режим активного обучения, при котором обьекты обучающей выборки можно подавать даже с нескольких машин по сети\n",
    "\n",
    "* Обучение может быть распараллелено на несколько машин\n",
    "\n",
    "### Установка\n",
    "\n",
    "* Ubuntu - ```apt-get install vowpal-wabbit```\n",
    "* Mac OS - ```port install vowpal_wabbit```\n",
    "* Windows - скачать установочник [тут](https://github.com/eisber/vowpal_wabbit/releases)\n",
    "\n",
    "### Формат данных\n",
    "\n",
    "Label [weight] |Namespace Feature ... |Namespace ...\n",
    "\n",
    "* ```Label``` - метка класса для задачи классификации или действительное число для задачи регрессии\n",
    "* ```weight``` - вес объекта, по умолчанию у всех 1\n",
    "* ```Namespace``` - все признаки разбиты на области видимости, может использоваться для раздельного использования или создания квадратичных признаков между областями\n",
    "* ```Feature``` - ```string[:value]``` или ```int[:value]``` строки будут хешированы, числа будут использоваться как индекс в векторе признаков. ```value``` по умолчанию равно $1$\n",
    "\n",
    "### Параметры\n",
    "\n",
    "**Hashing trick**\n",
    "\n",
    "Вводится функция $h$, с помощью которой получается индекс для записи значения в вектор признаков объекта.\n",
    "\n",
    "$$h : F \\rightarrow \\{0, \\dots, 2^b - 1\\}$$\n",
    "\n",
    "С помощью ```--b``` можно задавать размер области значений хеш-функции. Чем больше значение ```b```, тем меньше вероятность получить коллизии при хешировании признаков.\n",
    "\n",
    "**Оптимизация**\n",
    "\n",
    "Может использовать ```SGD``` или ```L-BFGS``` (квази-ньютоновский метод второго порядка, подробнее про работу [оптимизации](http://aria42.com/blog/2014/12/understanding-lbfgs))\n",
    "\n",
    "По умолчанию используется ```SGD```. ```L-BFGS``` включается с помощью ```--bfgs```, работает гораздо медленнее и подходит только для выборок небольшого размера. \n",
    "\n",
    "Количество проходов по данным для ```SGD``` задаётся с помощью параметра ```--passes```\n",
    "\n",
    "**Параметры оптимизации**\n",
    "\n",
    "Обновление весов происходит на каждом объекте:\n",
    "<br/>\n",
    "\n",
    "$$w_{t+1} = w_{t} + \\eta_t \\nabla_{w}\\ell(w_{t}, x_{t})$$\n",
    "$$\\eta_t = \\lambda d^k \\left( \\frac{t_0}{t_0 + t} \\right)^p$$\n",
    "\n",
    "где $t$ - номер объекта при обучении, $k$ - номер эпохи. Остальные параметры задаются следующим образом:\n",
    "\n",
    "* $\\lambda$: ```-l```\n",
    "* $d$: ```--decay_learning_rate```\n",
    "* $t_0$: ```--initial_t```\n",
    "* $p$: ```--power_t```\n",
    "\n",
    "**Функция потерь** задаётся через ```--loss_function```\n",
    "\n",
    "**Регуляризация** задаётся через два флага ```--l1``` и ```--l2```\n",
    "\n",
    "**Квадратичные признаки**\n",
    "\n",
    "* ```-q ab``` — создаёт квадратичные признаки, перемножая все признаки из областей видимости, названия которых начинаются на букву __a__ и на букву __b__\n",
    "* ```--ignore a``` — игнорирует все признаки из области видимости, название которой начинается на букву __a__\n",
    "\n",
    "**Режим демона**\n",
    "\n",
    "* ```--daemon``` — запускает __vw__ в режиме сервиса на порту, который можно задать с помощью ```--port```\n",
    "* Позволяет обучать модель и/или применять модель по сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num weight bits = 18\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "using no cache\r\n",
      "Reading datafile = \r\n",
      "num sources = 1\r\n",
      "\r\n",
      "\r\n",
      "VW options:\r\n",
      "  --random_seed arg                     seed random number generator\r\n",
      "  --ring_size arg                       size of example ring\r\n",
      "\r\n",
      "Update options:\r\n",
      "  -l [ --learning_rate ] arg            Set learning rate\r\n",
      "  --power_t arg                         t power value\r\n",
      "  --decay_learning_rate arg             Set Decay factor for learning_rate \r\n"
     ]
    }
   ],
   "source": [
    "!vw -h | head -n10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_vw = pd.DataFrame({'class': Y-80, 'description':[' '.join(re.findall(r'\\w+', d)) \n",
    "                                                      for d in description_lemma]})\n",
    "data_train, data_test = train_test_split(data_vw)\n",
    "data_train.to_csv('data.train.vw', sep='|', header=None, index=False)\n",
    "data_test.to_csv('data.test.vw', sep='|', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!vw -d data.train.vw  -f model.vw --loss_function logistic --oaa 21 --quiet --passes 100 -c -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```-d``` — откуда брать данны для обучения\n",
    "* ```-f``` — куда сохранять модель\n",
    "* ```--passes``` — максимальное количество проходов по выборке\n",
    "* ```-c``` — создавать файл с кешем, необходимо указывать, если используется ```--passes```\n",
    "* ```-k``` — перед запуском очищать кеш"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!vw -d data.test.vw -i model.vw -r output.csv --loss_function logistic --quiet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```-i``` — путь до готовой модели\n",
    "* ```-t``` — не обучаться, только вернуть предсказания\n",
    "* ```-p``` — путь до файла для записи предсказаний\n",
    "* ```--quite``` — не выводить никакую информацию в консоль"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
